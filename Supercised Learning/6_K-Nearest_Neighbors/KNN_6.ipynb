{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z8n6CKSgKVg"
      },
      "source": [
        "# **The k-Nearest Neighbors (KNN) Algorithm**\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "The k-Nearest Neighbors (KNN) algorithm is a highly regarded method within the realm of machine learning, known for its application in both classification and regression tasks. As an instance-based learning strategy, KNN operates on the premise that objects in close proximity to one another are likely to share similar outcomes. This non-parametric and lazy learning algorithm bases its predictions on the nearest neighbors within the training dataset, without necessitating a distinct training phase beforehand.\n",
        "\n",
        "### k-NN Algorithm\n",
        "\n",
        "The k-Nearest Neighbors algorithm can be summarized as follows:\n",
        "\n",
        "1. **Initialization:** It begins with a pre-labeled training dataset alongside a new, unlabeled point that requires classification or value prediction.\n",
        "\n",
        "2. **Distance Calculation:** The algorithm computes the distance between the new point and all points in the training dataset utilizing a specific distance metric, such as Euclidean distance, to gauge closeness.\n",
        "\n",
        "3. **Neighbor Selection:** It identifies the 'k' closest points (neighbors) to the new point based on the calculated distances.\n",
        "\n",
        "4. **Outcome Prediction:**\n",
        "    - In classification tasks, KNN assigns a class to the new point based on the most common class among its 'k' nearest neighbors.\n",
        "    \n",
        "    - For regression tasks, it predicts a value by averaging the values of the 'k' nearest neighbors.\n",
        "\n",
        "## Advantages and Disadvantages\n",
        "\n",
        "### Advantages\n",
        "\n",
        "1. **Simplicity**: The straightforwardness of KNN makes it easily understandable and implementable, serving as a powerful tool for analysts at all skill levels.\n",
        "\n",
        "2. **No Training Requirement**: Being a lazy learner, KNN is particularly suited for scenarios with dynamically changing datasets as it requires no explicit training phase.\n",
        "\n",
        "3. **Versatility**: KNN is adept at handling various types of data for classification, regression, and even anomaly detection.\n",
        "\n",
        "4. **Adaptability**: The algorithm is capable of adapting to different data distributions and capturing non-linear relationships.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "1. **Computational Demand**: The need to compute distances to all training points for each prediction renders KNN computationally intensive, especially with large datasets.\n",
        "\n",
        "2. **Sensitivity to k**: The performance of KNN heavily depends on the chosen 'k' value, necessitating careful selection.\n",
        "\n",
        "3. **Feature Scaling Necessity**: Given KNN's reliance on distance calculations, appropriate scaling of features is crucial to avoid distortions.\n",
        "\n",
        "4. **Curse of Dimensionality**: KNN's efficiency may decrease in high-dimensional spaces due to sparse data distribution and increased computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Overview\n",
        "\n",
        "For our exploration, we pivot from the Swiss Roll dataset to the \"Life Expectancy of the World\" dataset. This rich dataset provides a comprehensive look at life expectancy metrics across various countries, serving as a foundation for understanding global health trends. Unlike the Swiss Roll's three-dimensional, continuous, and non-linear manifold characteristics, the Life Expectancy dataset offers a real-world scenario where life expectancy figures are tied to specific countries and their geographical locations.\n",
        "\n",
        "### Dataset Characteristics\n",
        "1. **Dimensionality:** The dataset spans multiple dimensions but focuses on life expectancy figures for overall, male, and female populations in each country.\n",
        "\n",
        "2. **Geographical Association:** Each country's data point is associated with a continent, introducing a categorical dimension that facilitates classification tasks.\n",
        "\n",
        "3. **Real-World Application:** The dataset's real-world applicability extends to public health analysis, demographic studies, and geographic classification tasks, offering a stark contrast to the Swiss Roll's abstract nature.\n",
        "\n",
        "4. **Analytical Potential:** The life expectancy figures allow for regression analysis to predict life expectancy based on various factors, while the country-to-continent mapping supports classification tasks.\n",
        "\n",
        "### Implementing K-Nearest Neighbors (KNN) with the Dataset\n",
        "\n",
        "The objective is to use KNN not for unfolding a manifold but for predicting a country's continent based on its life expectancy metrics, showcasing the algorithm's versatility beyond theoretical examples to tangible, impactful analyses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Life Expectancy dataset\n",
        "data = pd.read_csv(Life_expectancy_dataset.csv)\n",
        "\n",
        "# Prepare the dataset\n",
        "features = data[['Overall Life', 'Male Life', 'Female Life']]\n",
        "target = data['Continent']\n",
        "\n",
        "# Encode categorical data and split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
        "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
        "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
