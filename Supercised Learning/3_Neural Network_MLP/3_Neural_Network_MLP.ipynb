{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4bC3P8wuSR7"
      },
      "source": [
        "# **Neural Network - Understanding Multilayer Perceptrons**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIrhDeKyuT9K"
      },
      "source": [
        "## **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u70fxYoQusLQ"
      },
      "source": [
        "Multilayer Perceptrons, or MLPs, stand as a cornerstone in the realm of deep learning. These networks, characterized by their layered structure of neurons, are adept at capturing and modeling the intricate relationships within data. Trained through the backpropagation algorithm, MLPs adjust their internal parameters to excel in tasks such as classification, regression, and feature detection. Multilayer Perceptron is a sophisticated type of Artificial Neural Network (ANN) distinguished by its structured layering. This includes an initial input layer, several hidden layers, and a concluding output layer, with neurons in each layer fully connected to those in the adjacent layers through weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Operational Mechanics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Feedforward Process\n",
        "The journey of input data through an MLP begins at the input layer, proceeding linearly to the output layer in what is known as the feedforward process. This involves the computation of weighted sums of inputs at each neuron, augmented by an activation function—such as sigmoid, ReLU, or tanh—to introduce non-linearity and facilitate complex pattern recognition.\n",
        "\n",
        "### The Backpropagation Algorithm\n",
        "Critical to learning, the backpropagation algorithm calculates the network's performance error by contrasting predictions against actual targets. This error is then used to adjust the weights in the network through gradient descent, iteratively refining the model's predictions towards accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **The Learning Cycle**\n",
        "\n",
        "The learning cycle of an MLP is an iterative process encompassing initialization, forward passes for prediction, error calculation, and weight adjustment through backpropagation and gradient descent. This cycle repeats across multiple epochs or until the network's error rate stabilizes at a satisfactory level, signifying convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Evaluating Performance**\n",
        "\n",
        "Upon completion of training, the MLP's ability to generalize its learning to unseen data is assessed, providing insight into its predictive accuracy and model robustness.\n",
        "\n",
        "## **Stochastic Gradient Descent (SGD) in Weight Update**\n",
        "\n",
        "An important facet of MLP training is the application of SGD for weight updates. This involves shuffling the training data, partitioning it into manageable mini-batches, and conducting forward passes and backpropagation for each batch. The weights are updated according to the computed gradients and a set learning rate, progressively reducing the loss and steering towards model convergence.\n",
        "\n",
        "## **MLP: A Double-Edged Sword**\n",
        "\n",
        "### Advantages\n",
        "\n",
        "1. Versatility: MLPs are adept across a wide spectrum of tasks, from classification to regression and beyond.\n",
        "\n",
        "2. Complex Data Modeling: They excel in capturing and modeling complex, non-linear relationships within data.\n",
        "\n",
        "3. Feature Learning: MLPs can autonomously learn and extract relevant features from data.\n",
        "\n",
        "4. Scalability: The architecture supports scaling up with more layers and neurons to handle increased complexity.\n",
        "\n",
        "5. Framework Support: They enjoy robust support across major machine learning frameworks, facilitating ease of use.\n",
        "\n",
        "### Challenges\n",
        "\n",
        "1. Overfitting: MLPs can overfit to training data, especially when data is sparse or the architecture overly complex.\n",
        "\n",
        "2. Hyperparameter Tuning: Achieving optimal performance requires careful tuning of numerous hyperparameters.\n",
        "\n",
        "3. Computational Demand: Training deep MLPs can be resource-intensive and time-consuming.\n",
        "\n",
        "4. Data Preprocessing: Effective training often necessitates significant preprocessing of input data.\n",
        "\n",
        "5. Interpretability: Unraveling how MLPs make decisions can be complex, impacting model transparency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Delving into the MNIST Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST dataset, a staple in machine learning, comprises 70,000 images of handwritten digits, divided into 60,000 training and 10,000 testing samples. Each 28x28 pixel image is a grayscale representation of digits 0 through 9, serving as a benchmark for assessing the performance of learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Acquiring the MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizing TensorFlow and Keras, we effortlessly access the MNIST dataset, a cornerstone for neural network applications. This step marks our foray into the domain of machine learning, setting the stage for subsequent data handling and model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/f9/14/67e9b2b2379cb530c0412123a674d045eca387dfcfa7db1c0028857b0a66/tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
            "  Downloading tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
            "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl.metadata\n",
            "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Obtaining dependency information for h5py>=3.10.0 from https://files.pythonhosted.org/packages/8d/70/2b0b99507287f66e71a6b2e66c5ad2ec2461ef2c534668eef96c3b48eb6d/h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/db/ed/1df62b44db2583375f6a8a5e2ca5432bbdc3edb477942b9b7c848c720055/libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Obtaining dependency information for ml-dtypes~=0.3.1 from https://files.pythonhosted.org/packages/6e/a4/6aabb78f1569550fd77c74d2c1d008b502c8ce72776bd88b14ea6c182c9e/ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: packaging in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/f3/bf/26deba06a4c910a85f78245cac7698f67cedd7efe00d04f6b3e1b3506a59/protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/c1/0a/a8c0f403b2189f5d3e490778ead51924b56fa30a35f6e444b3702e28c8c8/grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata\n",
            "  Downloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Obtaining dependency information for tensorboard<2.17,>=2.16 from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.0.0 (from tensorflow)\n",
            "  Obtaining dependency information for keras>=3.0.0 from https://files.pythonhosted.org/packages/49/a0/76cddcebb573097bcbf532ce05bd5dee6f26a228fb0cd874039e7a5abc4b/keras-3.2.0-py3-none-any.whl.metadata\n",
            "  Downloading keras-3.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/3e/56/1b7ef816e448464a93da70296db237129910b4452d6b4582d5e23fb07880/tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Collecting rich (from keras>=3.0.0->tensorflow)\n",
            "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl.metadata\n",
            "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/cd/43/b971880e2eb45c0bee2093710ae8044764a89afe9620df34a231c6f0ecd2/namex-0.0.7-py3-none-any.whl.metadata\n",
            "  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/cc/b8/33127d52de868d2aabc14ec6f53cb2dffafd14c5c708f50d171552a3a451/optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
            "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/yanshaoyu/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
            "Downloading tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl (227.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.0/227.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, rich, keras, tensorflow\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-3.2.0 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.16.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Loading the MNIST dataset\n",
        "(train_X, train_y), (test_X, test_y) = keras.datasets.mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing for Optimal Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The preparation of the MNIST dataset for MLP processing involves flattening the image matrices into 784-element vectors and normalizing these vectors. Such preprocessing ensures uniform scaling of input data, a critical step towards achieving model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten and normalize the images\n",
        "train_X_flat = train_X.reshape(train_X.shape[0], -1) / 255.0\n",
        "test_X_flat = test_X.reshape(test_X.shape[0], -1) / 255.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constructing the MLP Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With a design aimed at flexibility, the MLP model is structured to adapt to varying layers and neuron counts. It encompasses an input layer, several hidden layers for intricate data representation learning, and an output layer for final classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Parameter Initialization\n",
        "\n",
        "The journey continues with the initialization of network parameters. Proper initialization sets a strong foundation for the network, impacting its ability to learn and converge towards optimal solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Initializes parameters (weights and biases) for a neural network based on given layer dimensions.\n",
        "\n",
        "    Arguments:\n",
        "    layer_dims -- list containing the dimensions of each layer in the network\n",
        "\n",
        "    Returns:\n",
        "    parameters -- dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(3)  # Ensure consistent random initialization\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)  # Number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing Forward Propagation\n",
        "\n",
        "Forward propagation is a crucial step where we compute the activation of each neuron in the network. This process starts at the input layer and progresses through each layer in the network until the output is generated at the final layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def forward_prop(X, parameters):\n",
        "    \"\"\"Implement forward propagation for the [LINEAR->SIGMOID] * (L-1) -> LINEAR -> SOFTMAX computation.\"\"\"\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # Number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> SIGMOID]*(L-1).\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        Z = np.dot(parameters['W' + str(l)], A_prev) + parameters['b' + str(l)]\n",
        "        A = sigmoid(Z)\n",
        "        caches.append((A_prev, parameters['W' + str(l)], parameters['b' + str(l)], Z))\n",
        "        \n",
        "    # Implement LINEAR -> SOFTMAX.\n",
        "    ZL = np.dot(parameters['W' + str(L)], A) + parameters['b' + str(L)]\n",
        "    AL = np.exp(ZL) / np.sum(np.exp(ZL), axis=0, keepdims=True)\n",
        "    \n",
        "    caches.append((A, parameters['W' + str(L)], parameters['b' + str(L)], ZL))\n",
        "    return AL, caches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Essence of Backpropagation and Training\n",
        "\n",
        "Backpropagation is where the real learning happens. It involves calculating the gradient of the loss function with respect to each weight and bias in the network by propagating the error backward through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"Compute the cross-entropy cost.\"\"\"\n",
        "    m = Y.shape[1]\n",
        "    cost = -np.sum(Y * np.log(AL)) / m\n",
        "    cost = np.squeeze(cost)  # To make sure your cost's shape is what we expect.\n",
        "    return cost\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"Update parameters using gradient descent.\"\"\"\n",
        "    L = len(parameters) // 2  # Number of layers in the neural network\n",
        "    \n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
        "    \n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization and Empirical Evaluation\n",
        "\n",
        "Visualizing predictions against actual labels can offer deep insights into the learning effectiveness and areas for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(X, y, predictions):\n",
        "    \"\"\"Visualize predictions with a sample of test images.\"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i in range(10):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.imshow(X[i].reshape(28,28), cmap='gray')\n",
        "        plt.title(f\"Pred: {predictions[i]}, True: {y[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Synthesis and Prospective Outlook\n",
        "\n",
        "By walking through the steps of initializing network parameters, implementing forward propagation, refining the model with backpropagation, and visualizing the outcomes, we delve into the essence of MLPs. This journey from theory to practical application underscores the transformative potential of neural networks in recognizing and interpreting complex patterns in data.\n",
        "\n",
        "Our exploration, exemplified by Python code snippets and hands-on experimentation, not only elucidates the operational mechanics of MLPs but also illuminates their application on the MNIST dataset. Through this systematic approach, we achieve a confluence of theory and practice, paving the way for future advancements in neural network applications and research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Transforming Penguin Data for Neural Analysis**\n",
        "\n",
        "Embarking on a journey through the realms of neural computation, we start by harnessing the rich diversity of the penguin dataset. Our initial step involves purifying the data, converting categorical variables into numerical form through encoding, and ensuring all features stand on common ground via standardization. The prepared dataset then undergoes a split, segregating into distinct training and testing sets, with the target labels undergoing a transformation to fit the neural network's expectations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Fetch and clean the dataset\n",
        "penguins = sns.load_dataset('penguins').dropna()\n",
        "\n",
        "# Convert categorical data\n",
        "penguins[['species', 'island', 'sex']] = penguins[['species', 'island', 'sex']].apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# Define features and labels\n",
        "X = penguins.drop('species', axis=1)\n",
        "y = penguins['species']\n",
        "\n",
        "# Standardization\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crafting a Neural Blueprint from Scratch\n",
        "\n",
        "With the data ready, we delve into constructing a neural framework, crafting the very sinews and neurons that constitute our model. This architecture, birthed from the necessity of comprehension, features a carefully calculated initialization of weights and biases, ensuring a balanced onset for learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_network(layers):\n",
        "    np.random.seed(42)\n",
        "    weights = {}\n",
        "    biases = {}\n",
        "\n",
        "    for i in range(1, len(layers)):\n",
        "        weights[i] = np.random.randn(layers[i-1], layers[i]) * np.sqrt(2. / layers[i-1])\n",
        "        biases[i] = np.zeros((1, layers[i]))\n",
        "    \n",
        "    return weights, biases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Neural Symphony: Forward Motion and Learning\n",
        "\n",
        "In the heart of our neural orchestra lies the forward propagation, a melody of activations flowing through layers, and backpropagation, where learning takes its true form. These processes encapsulate the essence of adapting to the subtle patterns hidden within our feathered friends' data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def forward_propagate(X, weights, biases):\n",
        "    activations = X\n",
        "    for i in range(len(weights)):\n",
        "        z = np.dot(activations, weights[i]) + biases[i]\n",
        "        activations = sigmoid(z)\n",
        "    return activations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Training: A Dance of Weights and Biases\n",
        "\n",
        "Our training saga unfolds as we iteratively refine our model, adjusting its parameters in a dance guided by the gradients of loss. With each epoch, our neural network edges closer to understanding the clandestine language of penguin classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(X_train, y_train, epochs, learning_rate):\n",
        "    for epoch in range(epochs):\n",
        "        for X, y in zip(X_train, y_train):\n",
        "            # Forward pass\n",
        "            activations = forward_propagate(X)\n",
        "            # Backward pass and updates\n",
        "            # Placeholder for backpropagation and updates\n",
        "    print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Our Creation\n",
        "\n",
        "As our training culminates, we stand at the precipice of evaluation, ready to unveil the accuracy of our neural endeavor. The moment of truth reveals the efficacy of our model, a testament to the journey from raw data to predictive prowess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(X_test, y_test):\n",
        "    predictions = [predict(x) for x in X_test]  # Placeholder for the predict function\n",
        "    accuracy = np.mean(predictions == y_test)\n",
        "    print(f\"Model Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Epilogue\n",
        "\n",
        "Through this expedition from data preprocessing to neural network implementation and evaluation, we've woven a tale of analytical discovery, exploring the hidden nuances of the penguin dataset. This journey not only highlights the intricacies of neural network operations but also showcases the potential locked within the dataset, now unlocked by our tailored model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
